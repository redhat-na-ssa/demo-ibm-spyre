apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/accelerator-name: ibm-spyre
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: '["ibm.com/spyre_pf"]'
    opendatahub.io/runtime-version: v0.10.1.1
    opendatahub.io/serving-runtime-scope: global
    opendatahub.io/template-display-name: vLLM Spyre AI Accelerator ServingRuntime
      for KServe
    opendatahub.io/template-name: vllm-spyre-runtime
    openshift.io/display-name: demo-model
  labels:
    opendatahub.io/dashboard: "true"
  name: granite-3-1-8b-instruct
  namespace: demo-ibm-spyre
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8080"
  containers:
  - args:
    - --port=8000
    - --model=/mnt/models
    - --served-model-name={{.Name}}
    command:
    - /bin/bash
    - -c
    - |
      source /etc/profile.d/ibm-aiu-setup.sh
      exec python3 -m vllm.entrypoints.openai.api_server "$@"
    env:
    - name: HF_HOME
      value: /tmp/hf_home
    - name: FLEX_COMPUTE
      value: SENTIENT
    - name: FLEX_DEVICE
      value: PF
    - name: TOKENIZERS_PARALLELISM
      value: "false"
    - name: DTLOG_LEVEL
      value: error
    - name: TORCH_SENDNN_LOG
      value: CRITICAL
    - name: VLLM_SPYRE_WARMUP_BATCH_SIZES
      value: "4"
    - name: VLLM_SPYRE_WARMUP_PROMPT_LENS
      value: "1024"
    - name: VLLM_SPYRE_WARMUP_NEW_TOKENS
      value: "256"
    image: registry.redhat.io/rhaiis/vllm-spyre-rhel9:3.2.2
    name: kserve-container
    ports:
    - containerPort: 8000
      protocol: TCP
    # volumeMounts:
    # - mountPath: /dev/shm
    #   name: shm
  multiModel: false
  supportedModelFormats:
  - autoSelect: true
    name: vLLM
  # volumes:
  # - emptyDir:
  #     medium: Memory
  #     sizeLimit: 2Gi
  #   name: shm
